# ğŸ§  AI Text Detection

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)
[![TensorFlow / Keras](https://img.shields.io/badge/TensorFlow-Keras-orange.svg)](https://www.tensorflow.org/)

> Distinguishing between human-written and AI-generated text using deep learning and natural language processing (NLP).

---

## ğŸ“„ Overview

**AI Text Detection** is a data science project that explores the ability to distinguish between texts written by humans and those generated by artificial intelligence models.  
Using multiple neural architectures â€” from simple dense networks to Transformer-based encoders â€” this project aims to identify stylistic and semantic patterns that reveal the authorship of a given text.

Developed as part of the **Data Science and Artificial Intelligence degree at Universidad PolitÃ©cnica de Madrid**, this project evaluates the performance, robustness, and generalization capabilities of modern text classification models.

---

## ğŸ¯ Objectives

- Develop and evaluate deep learning models for classifying texts by authorship (human vs AI).
- Compare classical vectorization techniques (Bag of Words, TF-IDF) with modern embedding-based representations.
- Assess model robustness when presented with unseen texts.
- Identify the most reliable architecture for real-world AI text detection applications.

---

## ğŸ§© Methodology

The workflow followed a **supervised learning approach**, including:

1. **Data Preprocessing**
   - Cleaning, normalization, and balancing of the dataset.
   - Equal number of human-written and AI-generated samples (from [Kaggle Dataset](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text)).

2. **Exploratory Data Analysis**
   - Analysis of word frequency, text length distribution, and stylistic features.
   - Extraction of sequence length and vocabulary size for model input configuration.

3. **Vectorization**
   - **Bag of Words (BoW)** â€“ basic token counting representation.
   - **TF-IDF with bigrams** â€“ term importance weighting.
   - **Integer Encoding** â€“ tokenized representation for deep networks.

4. **Model Design and Training**
   - Four architectures were trained and compared:
     | Model | Description | Vectorization | Accuracy (Test) |
     |:------|:-------------|:---------------|:----------------|
     | 1ï¸âƒ£ Simple Dense NN | Basic dense network with ReLU and Sigmoid layers | Bag of Words | **99.75%** |
     | 2ï¸âƒ£ Simple Dense NN | Same as above, using TF-IDF bigrams | TF-IDF | **99.69%** |
     | 3ï¸âƒ£ Transformer Encoder | Embedding + TransformerEncoder (2 heads) | Integer Encoding | **99.66%** |
     | 4ï¸âƒ£ Transformer Encoder + PositionalEmbedding | Adds positional context | Integer Encoding | **99.3%** |

5. **Evaluation**
   - Metrics: Accuracy.
   - Loss: Binary Crossentropy.
   - Validation with unseen human and AI-generated texts (e.g., Wikipedia, The Lord of the Rings, CNN articles, academic papers).

---

## ğŸ“Š Results

All models achieved **very high accuracy (>99%)** on the test set, showing strong learning capacity.  
However, when exposed to **unseen data**, only the Transformer-based models showed consistent and reliable behavior.

**Best Performing Model:**
- **Transformer Encoder with Positional Embedding**
  - Balanced performance across human and AI-generated samples.
  - Demonstrated robustness and generalization in real-world scenarios.

**Performance summary:**
- Dense models â†’ High accuracy, but limited semantic understanding.  
- Transformer models â†’ Slightly lower accuracy on test data, but significantly better generalization.

---

## ğŸ” Conclusions

- Transformer architectures outperform traditional dense models in detecting text authorship due to their ability to capture **contextual and positional semantics**.
- The **Positional Embedding Transformer** emerged as the most robust model overall.
- Future improvements could include:
  - Expanding vocabulary size.
  - Keeping stopwords during training.
  - Adding dense or MultiHeadAttention layers.
  - Generating synthetic AI texts to further balance the dataset.

---

## ğŸ§° Technologies Used

- **Python 3.10**
- **TensorFlow / Keras**
- **NumPy, Pandas, Scikit-learn**
- **Matplotlib, Seaborn**
- **Jupyter Notebook**

---

## ğŸ“ Repository Structure
```
AI-Text-Detection/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ AI_Human_reduced.csv
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ Memoria.pdf
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ model_1_history.pkl
â”‚ â”œâ”€â”€ model_1.keras
â”‚ â”œâ”€â”€ model_2_history.pkl
â”‚ â”œâ”€â”€ model_2.keras
â”‚ â”œâ”€â”€ model_3_history.pkl
â”‚ â”œâ”€â”€ model_3.keras
â”‚ â”œâ”€â”€ model_4_history.pkl
â”‚ â””â”€â”€ model_4.keras
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ AI_vs_Human.ipynb
â”‚ â””â”€â”€ AI_vs_Human.pdf
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ create_reduced_dataset.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

ğŸ“‚ The data/ folder contains a reduced, balanced version of the original dataset (from Kaggle) for reproducibility.
The complete dataset (~ 1 GB) can be obtained [here](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text).

## ğŸ“š References

1. Vaswani et al. (2023). *Attention Is All You Need* â€“ arXiv.  
2. Robertson, S. (2004). *Understanding Inverse Document Frequency: On Theoretical Arguments for IDF*.  
3. Gerami, S. (2024). *AI vs Human Text Dataset*. [Kaggle](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text).  
4. Almeida, F., & XexÃ©o, G. (2023). *Word Embeddings: A Survey* â€“ arXiv.  
5. Tolkien, J. R. R. (2005). *The Lord of the Rings*, HarperCollins.  
6. Collinson, S. (2015). *CNN: How Donald Trump Took the Republican Party by Storm*.  

---

## ğŸ‘¤ Author

**RaÃºl Andrino**  
Universidad PolitÃ©cnica de Madrid  
ğŸ“§ Contact: [raulandrino90@gmail.com]  
ğŸ“˜ Project for: *Data Science and Artificial Intelligence Degree*  

---

## âš–ï¸ License

This project is licensed under the **MIT License** â€” feel free to use, modify, and share for educational or research purposes.

---

â­ *If you find this project useful, consider giving it a star on GitHub!*
